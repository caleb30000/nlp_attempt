{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\"\n",
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry.',\n",
       " 'The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066.',\n",
       " 'Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace.',\n",
       " \"Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pick out seperate elements from the sentences\n",
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#punctuation removal\n",
    "import re\n",
    "\n",
    "#remove punctuation characters\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\",\" \",sentences[2])\n",
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Queen', 'Camilla', 'was', 'crowned', 'alongside', 'him', 'before', 'a', 'huge', 'parade', 'back', 'to', 'Buckingham', 'Palace']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Queen', 'Camilla', 'crowned', 'alongside', 'huge', 'parade', 'back', 'Buckingham', 'Palace']\n"
     ]
    }
   ],
   "source": [
    "#Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words('english')]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#a glimpse of the stop words in nltk's corpus\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\abrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') #download for lemmatization\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['queen', 'camilla', 'crown', 'alongsid', 'huge', 'parad', 'back', 'buckingham', 'palac']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Queen', 'Camilla', 'crowned', 'alongside', 'huge', 'parade', 'back', 'Buckingham', 'Palace']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#reduce words to their root form\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming Output:['wait', 'wait', 'studi', 'studi', 'comput']\n",
      "Lemmatization Output:['wait', 'waiting', 'study', 'studying', 'computer']\n"
     ]
    }
   ],
   "source": [
    "#Better illustration of the difference between stemming and lemmatization\n",
    "words2 = ['wait', 'waiting', 'studies', 'studying', 'computers']\n",
    "\n",
    "#Stemming\n",
    "stemmed_words2 = [PorterStemmer().stem(w) for w in words2]\n",
    "print(\"Stemming Output:{}\".format(stemmed_words2))\n",
    "\n",
    "#Lemmatization\n",
    "lemmatized_words = [WordNetLemmatizer().lemmatize(w) for w in words2]\n",
    "print(\"Lemmatization Output:{}\".format(lemmatized_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\abrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Queen', 'NNP'),\n",
       " ('Camilla', 'NNP'),\n",
       " ('crowned', 'VBD'),\n",
       " ('alongside', 'RB'),\n",
       " ('huge', 'JJ'),\n",
       " ('parade', 'NN'),\n",
       " ('back', 'RB'),\n",
       " ('Buckingham', 'NNP'),\n",
       " ('Palace', 'NNP')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tag each word with parts of speech\n",
    "pos_tag(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\abrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Queen/NNP)\n",
      "  (PERSON Camilla/NNP)\n",
      "  was/VBD\n",
      "  crowned/VBN\n",
      "  alongside/RB\n",
      "  him/PRP\n",
      "  before/IN\n",
      "  a/DT\n",
      "  huge/JJ\n",
      "  parade/NN\n",
      "  back/RB\n",
      "  to/TO\n",
      "  (PERSON Buckingham/NNP Palace/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ner_tree = ne_chunk(pos_tag(word_tokenize(sentences[2])))\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Millions/NNS\n",
      "  of/IN\n",
      "  people/NNS\n",
      "  across/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION UK/NNP)\n",
      "  and/CC\n",
      "  beyond/IN\n",
      "  have/VBP\n",
      "  celebrated/VBN\n",
      "  the/DT\n",
      "  coronation/NN\n",
      "  of/IN\n",
      "  King/NNP\n",
      "  (PERSON Charles/NNP III/NNP)\n",
      "  -/:\n",
      "  a/DT\n",
      "  symbolic/JJ\n",
      "  ceremony/NN\n",
      "  combining/VBG\n",
      "  a/DT\n",
      "  religious/JJ\n",
      "  service/NN\n",
      "  and/CC\n",
      "  pageantry/NN\n",
      "  ./.\n",
      "  The/DT\n",
      "  ceremony/NN\n",
      "  was/VBD\n",
      "  held/VBN\n",
      "  at/IN\n",
      "  (ORGANIZATION Westminster/NNP Abbey/NNP)\n",
      "  ,/,\n",
      "  with/IN\n",
      "  the/DT\n",
      "  King/NNP\n",
      "  becoming/VBG\n",
      "  the/DT\n",
      "  40th/CD\n",
      "  reigning/VBG\n",
      "  monarch/NN\n",
      "  to/TO\n",
      "  be/VB\n",
      "  crowned/VBN\n",
      "  there/RB\n",
      "  since/IN\n",
      "  1066/CD\n",
      "  ./.\n",
      "  (PERSON Queen/NNP Camilla/NNP)\n",
      "  was/VBD\n",
      "  crowned/VBN\n",
      "  alongside/RB\n",
      "  him/PRP\n",
      "  before/IN\n",
      "  a/DT\n",
      "  huge/JJ\n",
      "  parade/NN\n",
      "  back/RB\n",
      "  to/TO\n",
      "  (PERSON Buckingham/NNP Palace/NNP)\n",
      "  ./.\n",
      "  Here/RB\n",
      "  's/VBZ\n",
      "  how/WRB\n",
      "  the/DT\n",
      "  day/NN\n",
      "  of/IN\n",
      "  splendour/NN\n",
      "  and/CC\n",
      "  formality/NN\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  featured/VBD\n",
      "  customs/NNS\n",
      "  dating/VBG\n",
      "  back/RB\n",
      "  more/JJR\n",
      "  than/IN\n",
      "  1,000/CD\n",
      "  years/NNS\n",
      "  ,/,\n",
      "  unfolded/VBD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\"\n",
    "\n",
    "ner_tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Twitter/NNP)\n",
      "  (ORGANIZATION CEO/NNP Elon/NNP Musk/NNP)\n",
      "  arrived/VBD\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (FACILITY Staples/NNP Center/NNP)\n",
      "  in/IN\n",
      "  (GPE Los/NNP Angeles/NNP)\n",
      "  ,/,\n",
      "  (GPE California/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"Twitter CEO Elon Musk arrived at the Staples Center in Los Angeles, California.\"\n",
    "ner_tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print(ner_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
